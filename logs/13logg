Sender: LSF System <lsfadmin@lo-s4-005>
Subject: Job 1892665: <python training.py --impath ../jpeg_patch/ --inpickle util/training_data_03.pickle --epoch 500 --batch 1024 --lr 0.001 --logid 13 --write True --limit 100000 --ratio 0.8> in cluster <leonhard> Exited

Job <python training.py --impath ../jpeg_patch/ --inpickle util/training_data_03.pickle --epoch 500 --batch 1024 --lr 0.001 --logid 13 --write True --limit 100000 --ratio 0.8> was submitted from host <lo-login-01> by user <javedh> in cluster <leonhard> at Sun May  5 16:01:09 2019
Job was executed on host(s) <2*lo-s4-005>, in queue <gpu.24h>, as user <javedh> in cluster <leonhard> at Sun May  5 16:01:24 2019
</cluster/home/javedh> was used as the home directory.
</cluster/scratch/javedh/text2map> was used as the working directory.
Started at Sun May  5 16:01:24 2019
Terminated at Mon May  6 12:42:57 2019
Results reported at Mon May  6 12:42:57 2019

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python training.py --impath ../jpeg_patch/ --inpickle util/training_data_03.pickle --epoch 500 --batch 1024 --lr 0.001 --logid 13 --write True --limit 100000 --ratio 0.8
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   59501.06 sec.
    Max Memory :                                 4686 MB
    Average Memory :                             4258.51 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               3506.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   74517 sec.
    Turnaround time :                            74508 sec.

The output (if any) follows:

Traceback (most recent call last):
  File "training.py", line 162, in <module>
    for batch_idx, data in enumerate(train_loader):
  File "/cluster/apps/python/3.7.1/x86_64/lib64/python3.7/site-packages/torch/utils/data/dataloader.py", line 615, in __next__
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "/cluster/apps/python/3.7.1/x86_64/lib64/python3.7/site-packages/torch/utils/data/dataloader.py", line 615, in <listcomp>
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "/cluster/apps/python/3.7.1/x86_64/lib64/python3.7/site-packages/torch/utils/data/dataset.py", line 103, in __getitem__
    return self.dataset[self.indices[idx]]
  File "training.py", line 57, in __getitem__
    return im.float(), word_indexed.float(), y.float()
KeyboardInterrupt
