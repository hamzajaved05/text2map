Sender: LSF System <lsfadmin@lo-s4-039>
Subject: Job 1892660: <python training.py --impath ../jpeg_patch/ --inpickle util/training_data_03.pickle --epoch 500 --batch 2048 --lr 0.001 --logid 11 --write True --limit 50000 --ratio 0.8> in cluster <leonhard> Exited

Job <python training.py --impath ../jpeg_patch/ --inpickle util/training_data_03.pickle --epoch 500 --batch 2048 --lr 0.001 --logid 11 --write True --limit 50000 --ratio 0.8> was submitted from host <lo-login-01> by user <javedh> in cluster <leonhard> at Sun May  5 15:56:36 2019
Job was executed on host(s) <2*lo-s4-039>, in queue <gpu.24h>, as user <javedh> in cluster <leonhard> at Sun May  5 15:56:54 2019
</cluster/home/javedh> was used as the home directory.
</cluster/scratch/javedh/text2map> was used as the working directory.
Started at Sun May  5 15:56:54 2019
Terminated at Sun May  5 15:58:02 2019
Results reported at Sun May  5 15:58:02 2019

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python training.py --impath ../jpeg_patch/ --inpickle util/training_data_03.pickle --epoch 500 --batch 2048 --lr 0.001 --logid 11 --write True --limit 50000 --ratio 0.8
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   17.85 sec.
    Max Memory :                                 4169 MB
    Average Memory :                             2683.40 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               4023.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                16
    Run time :                                   85 sec.
    Turnaround time :                            86 sec.

The output (if any) follows:

Traceback (most recent call last):
  File "training.py", line 165, in <module>
    outputs = Network(im.to(device), inputs.to(device))
  File "/cluster/apps/python/3.7.1/x86_64/lib64/python3.7/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "training.py", line 91, in forward
    im = self.i_pool3(im)
  File "/cluster/apps/python/3.7.1/x86_64/lib64/python3.7/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/cluster/apps/python/3.7.1/x86_64/lib64/python3.7/site-packages/torch/nn/modules/pooling.py", line 148, in forward
    self.return_indices)
  File "/cluster/apps/python/3.7.1/x86_64/lib64/python3.7/site-packages/torch/_jit_internal.py", line 132, in fn
    return if_false(*args, **kwargs)
  File "/cluster/apps/python/3.7.1/x86_64/lib64/python3.7/site-packages/torch/nn/functional.py", line 425, in _max_pool2d
    input, kernel_size, stride, padding, dilation, ceil_mode)[0]
  File "/cluster/apps/python/3.7.1/x86_64/lib64/python3.7/site-packages/torch/nn/functional.py", line 417, in max_pool2d_with_indices
    return torch._C._nn.max_pool2d_with_indices(input, kernel_size, _stride, padding, dilation, ceil_mode)
RuntimeError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 7.93 GiB total capacity; 7.29 GiB already allocated; 104.56 MiB free; 55.00 KiB cached)
Sender: LSF System <lsfadmin@lo-s4-039>
Subject: Job 1892663: <python training.py --impath ../jpeg_patch/ --inpickle util/training_data_03.pickle --epoch 500 --batch 1024 --lr 0.001 --logid 11 --write True --limit 50000 --ratio 0.8> in cluster <leonhard> Exited

Job <python training.py --impath ../jpeg_patch/ --inpickle util/training_data_03.pickle --epoch 500 --batch 1024 --lr 0.001 --logid 11 --write True --limit 50000 --ratio 0.8> was submitted from host <lo-login-01> by user <javedh> in cluster <leonhard> at Sun May  5 15:58:57 2019
Job was executed on host(s) <2*lo-s4-039>, in queue <gpu.24h>, as user <javedh> in cluster <leonhard> at Sun May  5 15:59:23 2019
</cluster/home/javedh> was used as the home directory.
</cluster/scratch/javedh/text2map> was used as the working directory.
Started at Sun May  5 15:59:23 2019
Terminated at Sun May  5 16:12:20 2019
Results reported at Sun May  5 16:12:20 2019

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python training.py --impath ../jpeg_patch/ --inpickle util/training_data_03.pickle --epoch 500 --batch 1024 --lr 0.001 --logid 11 --write True --limit 50000 --ratio 0.8
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   184.25 sec.
    Max Memory :                                 3443 MB
    Average Memory :                             2738.14 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               4749.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                20
    Run time :                                   797 sec.
    Turnaround time :                            803 sec.

The output (if any) follows:

Traceback (most recent call last):
  File "training.py", line 180, in <module>
    outputs = Network(im.to(device), inputs.to(device))
  File "/cluster/apps/python/3.7.1/x86_64/lib64/python3.7/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "training.py", line 89, in forward
    im = F.relu(im)
  File "/cluster/apps/python/3.7.1/x86_64/lib64/python3.7/site-packages/torch/nn/functional.py", line 862, in relu
    result = torch.relu(input)
RuntimeError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 7.93 GiB total capacity; 7.29 GiB already allocated; 94.56 MiB free; 7.75 MiB cached)
Sender: LSF System <lsfadmin@lo-s4-039>
Subject: Job 1892721: <python training.py --impath ../jpeg_patch/ --inpickle util/training_data_03.pickle --epoch 500 --batch 1024 --lr 0.001 --logid 11 --write True --limit 50000 --ratio 0.8> in cluster <leonhard> Exited

Job <python training.py --impath ../jpeg_patch/ --inpickle util/training_data_03.pickle --epoch 500 --batch 1024 --lr 0.001 --logid 11 --write True --limit 50000 --ratio 0.8> was submitted from host <lo-login-01> by user <javedh> in cluster <leonhard> at Sun May  5 16:45:46 2019
Job was executed on host(s) <2*lo-s4-039>, in queue <gpu.24h>, as user <javedh> in cluster <leonhard> at Sun May  5 16:46:02 2019
</cluster/home/javedh> was used as the home directory.
</cluster/scratch/javedh/text2map> was used as the working directory.
Started at Sun May  5 16:46:02 2019
Terminated at Sun May  5 16:48:49 2019
Results reported at Sun May  5 16:48:49 2019

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python training.py --impath ../jpeg_patch/ --inpickle util/training_data_03.pickle --epoch 500 --batch 1024 --lr 0.001 --logid 11 --write True --limit 50000 --ratio 0.8
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   130.66 sec.
    Max Memory :                                 3155 MB
    Average Memory :                             2565.37 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5037.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                20
    Run time :                                   190 sec.
    Turnaround time :                            183 sec.

The output (if any) follows:

Traceback (most recent call last):
  File "training.py", line 180, in <module>
    outputs = Network(im.to(device), inputs.to(device))
  File "/cluster/apps/python/3.7.1/x86_64/lib64/python3.7/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "training.py", line 89, in forward
    im = F.relu(im)
  File "/cluster/apps/python/3.7.1/x86_64/lib64/python3.7/site-packages/torch/nn/functional.py", line 862, in relu
    result = torch.relu(input)
RuntimeError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 7.93 GiB total capacity; 7.29 GiB already allocated; 94.56 MiB free; 7.75 MiB cached)
Sender: LSF System <lsfadmin@lo-s4-039>
Subject: Job 1892734: <python training.py --impath ../jpeg_patch/ --inpickle util/training_data_03.pickle --epoch 500 --batch 1024 --lr 0.001 --logid 11 --write True --limit 50000 --ratio 0.8> in cluster <leonhard> Exited

Job <python training.py --impath ../jpeg_patch/ --inpickle util/training_data_03.pickle --epoch 500 --batch 1024 --lr 0.001 --logid 11 --write True --limit 50000 --ratio 0.8> was submitted from host <lo-login-01> by user <javedh> in cluster <leonhard> at Sun May  5 16:58:16 2019
Job was executed on host(s) <2*lo-s4-039>, in queue <gpu.24h>, as user <javedh> in cluster <leonhard> at Sun May  5 16:58:32 2019
</cluster/home/javedh> was used as the home directory.
</cluster/scratch/javedh/text2map> was used as the working directory.
Started at Sun May  5 16:58:32 2019
Terminated at Sun May  5 17:01:14 2019
Results reported at Sun May  5 17:01:14 2019

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python training.py --impath ../jpeg_patch/ --inpickle util/training_data_03.pickle --epoch 500 --batch 1024 --lr 0.001 --logid 11 --write True --limit 50000 --ratio 0.8
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   133.09 sec.
    Max Memory :                                 3117 MB
    Average Memory :                             2418.00 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5075.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                20
    Run time :                                   174 sec.
    Turnaround time :                            178 sec.

The output (if any) follows:

Traceback (most recent call last):
  File "training.py", line 180, in <module>
    outputs = Network(im.to(device), inputs.to(device))
  File "/cluster/apps/python/3.7.1/x86_64/lib64/python3.7/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "training.py", line 89, in forward
    im = F.relu(im)
  File "/cluster/apps/python/3.7.1/x86_64/lib64/python3.7/site-packages/torch/nn/functional.py", line 862, in relu
    result = torch.relu(input)
RuntimeError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 7.93 GiB total capacity; 7.29 GiB already allocated; 94.56 MiB free; 7.75 MiB cached)
Sender: LSF System <lsfadmin@lo-s4-039>
Subject: Job 1892738: <python training.py --impath ../jpeg_patch/ --inpickle util/training_data_03.pickle --epoch 500 --batch 512 --lr 0.001 --logid 11 --write True --limit 50000 --ratio 0.8> in cluster <leonhard> Exited

Job <python training.py --impath ../jpeg_patch/ --inpickle util/training_data_03.pickle --epoch 500 --batch 512 --lr 0.001 --logid 11 --write True --limit 50000 --ratio 0.8> was submitted from host <lo-login-01> by user <javedh> in cluster <leonhard> at Sun May  5 17:02:22 2019
Job was executed on host(s) <2*lo-s4-039>, in queue <gpu.24h>, as user <javedh> in cluster <leonhard> at Sun May  5 17:02:32 2019
</cluster/home/javedh> was used as the home directory.
</cluster/scratch/javedh/text2map> was used as the working directory.
Started at Sun May  5 17:02:32 2019
Terminated at Mon May  6 12:42:51 2019
Results reported at Mon May  6 12:42:51 2019

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python training.py --impath ../jpeg_patch/ --inpickle util/training_data_03.pickle --epoch 500 --batch 512 --lr 0.001 --logid 11 --write True --limit 50000 --ratio 0.8
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   57090.62 sec.
    Max Memory :                                 3131 MB
    Average Memory :                             2526.99 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               5061.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   70835 sec.
    Turnaround time :                            70829 sec.

The output (if any) follows:

Traceback (most recent call last):
  File "training.py", line 162, in <module>
    for batch_idx, data in enumerate(train_loader):
  File "/cluster/apps/python/3.7.1/x86_64/lib64/python3.7/site-packages/torch/utils/data/dataloader.py", line 615, in __next__
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "/cluster/apps/python/3.7.1/x86_64/lib64/python3.7/site-packages/torch/utils/data/dataloader.py", line 615, in <listcomp>
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "/cluster/apps/python/3.7.1/x86_64/lib64/python3.7/site-packages/torch/utils/data/dataset.py", line 103, in __getitem__
    return self.dataset[self.indices[idx]]
  File "training.py", line 56, in __getitem__
    im = torch.tensor(cv2.imread(self.im_path+jpgs[index][:-4]+"_"+words[index] + ".jpg")).view(3,128,256)
KeyboardInterrupt
