Sender: LSF System <lsfadmin@lo-s4-067>
Subject: Job 1892666: <python training.py --impath ../jpeg_patch/ --inpickle util/training_data_03.pickle --epoch 500 --batch 1024 --lr 0.0001 --logid 14 --write True --limit 100000 --ratio 0.8> in cluster <leonhard> Exited

Job <python training.py --impath ../jpeg_patch/ --inpickle util/training_data_03.pickle --epoch 500 --batch 1024 --lr 0.0001 --logid 14 --write True --limit 100000 --ratio 0.8> was submitted from host <lo-login-01> by user <javedh> in cluster <leonhard> at Sun May  5 16:01:29 2019
Job was executed on host(s) <2*lo-s4-067>, in queue <gpu.24h>, as user <javedh> in cluster <leonhard> at Sun May  5 16:01:54 2019
</cluster/home/javedh> was used as the home directory.
</cluster/scratch/javedh/text2map> was used as the working directory.
Started at Sun May  5 16:01:54 2019
Terminated at Mon May  6 12:42:42 2019
Results reported at Mon May  6 12:42:42 2019

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python training.py --impath ../jpeg_patch/ --inpickle util/training_data_03.pickle --epoch 500 --batch 1024 --lr 0.0001 --logid 14 --write True --limit 100000 --ratio 0.8
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   59180.21 sec.
    Max Memory :                                 4617 MB
    Average Memory :                             4216.62 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               3575.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   74467 sec.
    Turnaround time :                            74473 sec.

The output (if any) follows:

Traceback (most recent call last):
  File "training.py", line 162, in <module>
    for batch_idx, data in enumerate(train_loader):
  File "/cluster/apps/python/3.7.1/x86_64/lib64/python3.7/site-packages/torch/utils/data/dataloader.py", line 615, in __next__
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "/cluster/apps/python/3.7.1/x86_64/lib64/python3.7/site-packages/torch/utils/data/dataloader.py", line 232, in default_collate
    return [default_collate(samples) for samples in transposed]
  File "/cluster/apps/python/3.7.1/x86_64/lib64/python3.7/site-packages/torch/utils/data/dataloader.py", line 232, in <listcomp>
    return [default_collate(samples) for samples in transposed]
  File "/cluster/apps/python/3.7.1/x86_64/lib64/python3.7/site-packages/torch/utils/data/dataloader.py", line 209, in default_collate
    return torch.stack(batch, 0, out=out)
KeyboardInterrupt
