Sender: LSF System <lsfadmin@lo-s4-023>
Subject: Job 1890328: <python training.py --impath ../jpeg_patch/ --inpickle util/training_data_03.pickle --epoch 100 --batch 1024 --lr 0.02 --logid 04 --write True> in cluster <leonhard> Exited

Job <python training.py --impath ../jpeg_patch/ --inpickle util/training_data_03.pickle --epoch 100 --batch 1024 --lr 0.02 --logid 04 --write True> was submitted from host <lo-login-01> by user <javedh> in cluster <leonhard> at Sat May  4 13:48:49 2019
Job was executed on host(s) <2*lo-s4-023>, in queue <gpu.24h>, as user <javedh> in cluster <leonhard> at Sat May  4 13:49:08 2019
</cluster/home/javedh> was used as the home directory.
</cluster/scratch/javedh/text2map> was used as the working directory.
Started at Sat May  4 13:49:08 2019
Terminated at Sat May  4 19:54:05 2019
Results reported at Sat May  4 19:54:05 2019

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python training.py --impath ../jpeg_patch/ --inpickle util/training_data_03.pickle --epoch 100 --batch 1024 --lr 0.02 --logid 04 --write True
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   15219.19 sec.
    Max Memory :                                 4494 MB
    Average Memory :                             3993.13 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               3698.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                17
    Run time :                                   21922 sec.
    Turnaround time :                            21916 sec.

The output (if any) follows:



Data Loaded


Dataset accuracy >> 625, Epoch 1, loss > 0.025731926483202956
Dataset accuracy >> 1153, Epoch 2, loss > 0.00858132426934234
Dataset accuracy >> 1410, Epoch 3, loss > 0.008444687633539994
Dataset accuracy >> 1682, Epoch 4, loss > 0.008359813136661691
Dataset accuracy >> 1666, Epoch 5, loss > 0.008323702594030397
Dataset accuracy >> 1668, Epoch 6, loss > 0.008313560910486285
Dataset accuracy >> 1627, Epoch 7, loss > 0.008305184736246083
Dataset accuracy >> 1663, Epoch 8, loss > 0.008260882457788117
Dataset accuracy >> 1744, Epoch 9, loss > 0.008264036343069825
Dataset accuracy >> 1683, Epoch 10, loss > 0.00826222422001078
Dataset accuracy >> 1740, Epoch 11, loss > 0.008257189373514882
Dataset accuracy >> 1698, Epoch 12, loss > 0.00827402187632908
Dataset accuracy >> 1588, Epoch 13, loss > 0.008291628663517495
Dataset accuracy >> 1661, Epoch 14, loss > 0.008323155347154342
Dataset accuracy >> 1535, Epoch 15, loss > 0.008368654845789086
Dataset accuracy >> 1527, Epoch 16, loss > 0.008344272175601793
Dataset accuracy >> 1592, Epoch 17, loss > 0.008383451349715002
Dataset accuracy >> 1359, Epoch 18, loss > 0.008487568482208711
Dataset accuracy >> 1356, Epoch 19, loss > 0.008471623699072001
Dataset accuracy >> 1446, Epoch 20, loss > 0.008496265334050505
Dataset accuracy >> 1010, Epoch 21, loss > 0.008691230218287965
Dataset accuracy >> 863, Epoch 22, loss > 0.00871111480713484
Dataset accuracy >> 926, Epoch 23, loss > 0.008647328257182906
Dataset accuracy >> 943, Epoch 24, loss > 0.008621766938146837
Dataset accuracy >> 945, Epoch 25, loss > 0.008633135242607169
Dataset accuracy >> 886, Epoch 26, loss > 0.008648754628818298
Dataset accuracy >> 889, Epoch 27, loss > 0.00861220553211084
Dataset accuracy >> 888, Epoch 28, loss > 0.008716462301891317
Dataset accuracy >> 909, Epoch 29, loss > 0.008652705046053625
Traceback (most recent call last):
  File "training.py", line 146, in <module>
    los = []
  File "/cluster/apps/python/3.7.1/x86_64/lib64/python3.7/site-packages/torch/utils/data/dataloader.py", line 615, in __next__
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "/cluster/apps/python/3.7.1/x86_64/lib64/python3.7/site-packages/torch/utils/data/dataloader.py", line 615, in <listcomp>
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "training.py", line 58, in __getitem__
    def __len__(self):
KeyboardInterrupt
